{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "witsDpl-qOLa"
      },
      "source": [
        "# Crimerate Classification - San Francisco\n",
        "\n",
        "## Abstract\n",
        "### 1. Problem Statement:\n",
        "The main objective of this project is to implement Big Data technologies in the machine learning realm. As part of this project, we will be working on the San Francisco Crime Classification dataset obtained from Kaggle. We are mainly interested in developing a system that could classify crime descriptions into different categories which would help the authorities to assign officers to crimes based on the report.\n",
        "\n",
        "### 2. Solution:\n",
        "There can be numerous approaches to solving this problem. Out of all those approaches we will be using the crime dataset and working around it. We will train a model based on 39 predefined categories, test its accuracy, and deploy it into production. Given a new crime description, the system should assign it to one of the 39 categories. In addition, to solve this multi-class text classification problem, we will use various feature extraction techniques along with different supervised machine learning algorithms in Pyspark.\n",
        "\n",
        "### 3. Project Goals:\n",
        "We will try different sets of models to check the crime rate and compare their accuracy. This comparative analysis would help us know which model would be the best for this kind of dataset and problem.\n",
        "\n",
        "### 4. References :\n",
        "- [Kaggle](https://www.kaggle.com/datasets/kaggle/san-francisco-crime-classification)\n",
        "- [Researchgate](https://www.researchgate.net/publication/347219439_Crime_Rate_Prediction_Using_Machine_Learning_and_Data_Mining)\n",
        "- [IEEE Explore](https://ieeexplore.ieee.org/document/9170731)\n",
        "\n",
        "- [NCBI](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8529125/)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "fdjmk2UXq7L-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "vykb66teqOLd",
        "outputId": "99b40e8e-d7d5-46fd-ba01-aec75f89b707"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    121\u001b[0m       'TBE_EPHEM_CREDS_ADDR'] if ephemeral else _os.environ['TBE_CREDS_ADDR']\n\u001b[1;32m    122\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    124\u001b[0m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install geopandas --quiet\n",
        "! pip install --upgrade plotly --quiet\n",
        "! pip install mpu --quiet\n",
        "! pip install visualization --quiet"
      ],
      "metadata": {
        "id": "htHo9OaWqj0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vWSPnWEFq4eE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "from mpu import haversine_distance\n",
        "from plotly.subplots import make_subplots\n",
        "from tqdm import tqdm\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_extraction.text import (\n",
        "    CountVectorizer,\n",
        "    TfidfVectorizer\n",
        ")\n",
        "\n",
        "from visualizer import (\n",
        "    MapScatter,\n",
        "    MapChoropleth,\n",
        "    OccurrencePlotter,\n",
        "    CategoryOccurrencePlotter\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "mGNMo7OqqxRK",
        "outputId": "a1f45ce7-0ddd-4e1f-82e9-1fd6d3717809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-cede89598827>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_path = '/content/drive/MyDrive/AAIC/SCS-1/sf_crime_classification/'"
      ],
      "metadata": {
        "id": "pG7fkNT9sFEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sf_df = pd.read_csv(filepath_or_buffer=project_path + 'csv_files/train.csv')\n",
        "test_sf_df = pd.read_csv(filepath_or_buffer=project_path + 'csv_files/test.csv')\n",
        "sf_pd = gpd.read_file(filename=project_path + 'shp_files/sf-police-districts/sf-police-districts.shp')"
      ],
      "metadata": {
        "id": "6q3r99wUsMLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sf_df.shape, test_sf_df.shape"
      ],
      "metadata": {
        "id": "rMr32LjCsTFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sf_df.head(2)"
      ],
      "metadata": {
        "id": "-hgUIck8sUK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sf_df.head(2)"
      ],
      "metadata": {
        "id": "mpncNUxTsaa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cols_renamed = ['time', 'category', 'description', 'weekday', 'police_dept', \n",
        "                      'resolution', 'address', 'longitude', 'latitude']\n",
        "train_sf_df.columns = train_cols_renamed\n",
        "\n",
        "test_cols_renamed = ['id', 'time', 'weekday', 'police_dept', 'address', 'longitude', 'latitude']\n",
        "test_sf_df.columns = test_cols_renamed"
      ],
      "metadata": {
        "id": "7NcxBjhvshao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sf_df.drop(columns=['description', 'resolution'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "uGj-HQAXskw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sf_df.head(2)"
      ],
      "metadata": {
        "id": "cYnctn2QstK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sf_df.head(2)"
      ],
      "metadata": {
        "id": "HZ68H8QAsvkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sf_df.dtypes"
      ],
      "metadata": {
        "id": "lm49hY9Msx7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sf_df.dtypes"
      ],
      "metadata": {
        "id": "NYgiouJos0YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sf_df.dtypes"
      ],
      "metadata": {
        "id": "gi4Io4gvs3b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_date(time):\n",
        "    \"\"\"Extract data from time\"\"\"\n",
        "    return time.split(' ')[0]\n",
        "\n",
        "def extract_year(date):\n",
        "    \"\"\"Extract year from date\"\"\"\n",
        "    return int(date.split('-')[0])\n",
        "\n",
        "def extract_month(date):\n",
        "    \"\"\"Extract month from date\"\"\"\n",
        "    return int(date.split('-')[1])\n",
        "\n",
        "def extract_day(date):\n",
        "    \"\"\"Extract day from date\"\"\"\n",
        "    return int(date.split('-')[2])\n",
        "\n",
        "def extract_hour(time):\n",
        "    \"\"\"Extract hour from time\"\"\"\n",
        "    date, hms = time.split(' ')\n",
        "    return int(hms.split(':')[0])\n",
        "\n",
        "def extract_minute(time):\n",
        "    \"\"\"Extract minute from time\"\"\"\n",
        "    date, hms = time.split(' ')\n",
        "    return int(hms.split(':')[1])\n",
        "\n",
        "def extract_season(month):\n",
        "    \"\"\"Determine season from month\"\"\"\n",
        "    if month in [4, 5, 6]:\n",
        "        return 'summer'\n",
        "    elif month in [7, 8, 9]:\n",
        "        return 'rainy'\n",
        "    elif month in [10, 11, 12]:\n",
        "        return 'winter'\n",
        "    return 'spring'\n",
        "\n",
        "def extract_hour_type(hour):\n",
        "    \"\"\"Determine hour type from hour\"\"\"\n",
        "    if (hour >= 4) and (hour < 12):\n",
        "        return 'morning'\n",
        "    elif (hour >= 12) and (hour < 15):\n",
        "        return 'noon'\n",
        "    elif (hour >= 15) and (hour < 18):\n",
        "        return 'evening'\n",
        "    elif (hour >= 18) and (hour < 22):\n",
        "        return 'night'\n",
        "    return 'mid-night'\n",
        "\n",
        "def extract_time_period(hour):\n",
        "    \"\"\"Determine the time period from hour\"\"\"\n",
        "    if hour in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]:\n",
        "        return 'am'\n",
        "    return 'pm'"
      ],
      "metadata": {
        "id": "AEm0f0A8s5si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def title_text(text):\n",
        "    \"\"\"Title the text\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        text = text.title()\n",
        "        return text\n",
        "    return text"
      ],
      "metadata": {
        "id": "bb9Q5US5s72h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_address_type(addr):\n",
        "    \"\"\"Extract address type if it Street or Cross etc\"\"\"\n",
        "    if ' / ' in addr:\n",
        "        return 'Cross'\n",
        "    addr_sep = addr.split(' ')\n",
        "    addr_type = addr_sep[-1]\n",
        "    return addr_type"
      ],
      "metadata": {
        "id": "zRJz5eNhs93O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_temporal_address_features(df, path):\n",
        "    \"\"\"Writing the temporal based features\"\"\"\n",
        "    \n",
        "    ### Adding temporal features\n",
        "    df['date'] = df['time'].apply(func=extract_date)\n",
        "    df['year'] = df['date'].apply(func=extract_year)\n",
        "    df['month'] = df['date'].apply(func=extract_month)\n",
        "    df['day'] = df['date'].apply(func=extract_day)\n",
        "    df['hour'] = df['time'].apply(func=extract_hour)\n",
        "    df['minute'] = df['time'].apply(func=extract_minute)\n",
        "    df['season'] = df['month'].apply(func=extract_season)\n",
        "    df['hour_type'] = df['hour'].apply(func=extract_hour_type)\n",
        "    df['time_period'] = df['hour'].apply(func=extract_time_period)\n",
        "    \n",
        "    ### Adding address type\n",
        "    df['address_type'] = df['address'].apply(func=extract_address_type)\n",
        "    \n",
        "    ### Text titling\n",
        "    df = df.applymap(func=title_text)\n",
        "    \n",
        "    ### Writing\n",
        "    df.to_csv(path_or_buf=path, index=None)\n",
        "    \n",
        "    return True"
      ],
      "metadata": {
        "id": "RV9PjHMttA_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (\n",
        "    not os.path.isfile(path=project_path + 'csv_files/train_time_address_cleaned.csv') and\n",
        "    not os.path.isfile(path=project_path + 'csv_files/test_time_address_cleaned.csv')\n",
        "   ):\n",
        "    # Training\n",
        "    write_temporal_address_features(df=train_sf_df, path=project_path + 'csv_files/train_time_address_cleaned.csv')\n",
        "    # Test\n",
        "    write_temporal_address_features(df=test_sf_df, path=project_path + 'csv_files/test_time_address_cleaned.csv')\n",
        "\n",
        "else:\n",
        "    print(\"Data already exists in the directory.\")\n",
        "    train_sf_df = pd.read_csv(filepath_or_buffer=project_path + 'csv_files/train_time_address_cleaned.csv')\n",
        "    test_sf_df = pd.read_csv(filepath_or_buffer=project_path + 'csv_files/test_time_address_cleaned.csv')"
      ],
      "metadata": {
        "id": "bLHsBZA-tD1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sf_df.head(2)"
      ],
      "metadata": {
        "id": "lCWsOsw8tGyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sf_df.head(2)"
      ],
      "metadata": {
        "id": "cy79pqTAtIhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sf_df[['latitude', 'longitude']].describe()"
      ],
      "metadata": {
        "id": "CEqbACdPtMCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sf_df[['latitude', 'longitude']].describe()"
      ],
      "metadata": {
        "id": "-eDvmD4WtN6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_column_distribution(df, column):\n",
        "    \"\"\"Plot the distribution of the column from dataframe\"\"\"\n",
        "    \n",
        "    column_val_df = df[column].value_counts().to_frame().reset_index()\n",
        "    column_val_df.columns = [column, 'count']\n",
        "    \n",
        "    fig = px.bar(data_frame=column_val_df, x=column, y='count')\n",
        "    fig.update_layout(\n",
        "        autosize=True,\n",
        "        height=600,\n",
        "        hovermode='closest',\n",
        "        showlegend=True,\n",
        "        margin=dict(l=10, r=10, t=30, b=0)\n",
        "    )\n",
        "    \n",
        "    fig.show()\n",
        "    return None"
      ],
      "metadata": {
        "id": "-uGpWbwFtRKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_column_distribution(df=train_sf_df, column='category')"
      ],
      "metadata": {
        "id": "ANaDPWXntVa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_column_distribution(df=train_sf_df, column='address_type')"
      ],
      "metadata": {
        "id": "CfRfuVQ4tXDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_column_distribution(df=train_sf_df, column='police_dept')"
      ],
      "metadata": {
        "id": "5KRNWdS9tZbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_column_distribution(df=train_sf_df, column='year')"
      ],
      "metadata": {
        "id": "VOntxnS7tbeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_column_distribution(df=train_sf_df, column='month')"
      ],
      "metadata": {
        "id": "Nou_W09lteQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_column_distribution(df=train_sf_df, column='weekday')"
      ],
      "metadata": {
        "id": "Kw4qkmB_tgbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_column_distribution(df=train_sf_df, column='hour')"
      ],
      "metadata": {
        "id": "l_r_CqGMtmOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_column_distribution(df=train_sf_df, column='minute')"
      ],
      "metadata": {
        "id": "gEwFEq5NtpCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_column_distribution(df=train_sf_df, column='season')"
      ],
      "metadata": {
        "id": "Wk2vbTWftrIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_column_distribution(df=train_sf_df, column='time_period')"
      ],
      "metadata": {
        "id": "abXrvM2Stspt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_column_distribution(df=train_sf_df, column='hour_type')"
      ],
      "metadata": {
        "id": "Mu4qu7eCtxxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oviz = OccurrencePlotter(df=train_sf_df)"
      ],
      "metadata": {
        "id": "gHTF7nw5t0Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oviz.plot_crime_occurrences(police_dept='Southern')"
      ],
      "metadata": {
        "id": "fkM1w9K1t1vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oviz.plot_crime_occurrences_by_year(year=2003, police_dept='Southern')"
      ],
      "metadata": {
        "id": "4R-j-SNwt39f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oviz.plot_crime_occurrences_by_month(year=2003, month=1, police_dept='Southern')"
      ],
      "metadata": {
        "id": "nTv1jii7t6Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# oviz.plot_crime_occurrences_by_day(year=2005, month=1, day=10)"
      ],
      "metadata": {
        "id": "okkQQOGVt8G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mviz = MapScatter(df=train_sf_df)"
      ],
      "metadata": {
        "id": "TP9Benbft-To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mviz.map_crimes(police_dept='Richmond')"
      ],
      "metadata": {
        "id": "-fXG-95nuADh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mviz.map_crimes_by_year(year=2015, police_dept='Richmond')"
      ],
      "metadata": {
        "id": "rLi04u24uB1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mviz.map_crimes_by_month(year=2003, month=2, police_dept='Richmond')"
      ],
      "metadata": {
        "id": "V1gVDGo1uFeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mviz.map_crimes_by_day(year=2003, month=2, day=6)"
      ],
      "metadata": {
        "id": "c0Y5XGPLuG5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mciz = MapChoropleth(df=train_sf_df, gdf=sf_pd)"
      ],
      "metadata": {
        "id": "92Kf9ebEuKTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mciz.map_crimes()"
      ],
      "metadata": {
        "id": "5LfMdb8ouMsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mciz.map_crimes_by_year(year=2015)"
      ],
      "metadata": {
        "id": "x80a-Wg3uOgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mciz.map_crimes_by_month(year=2015, month=3)"
      ],
      "metadata": {
        "id": "FQgxcGSbuQch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mciz.map_crimes_by_day(year=2015, month=3, day=3)"
      ],
      "metadata": {
        "id": "O1testjGuSkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cop = CategoryOccurrencePlotter(df=train_sf_df)"
      ],
      "metadata": {
        "id": "5hFMSloJuSrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cop.plot_crime_occurrences_by_month()"
      ],
      "metadata": {
        "id": "pi8j8HXyuZO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cop.plot_crime_occurrences_by_weekday()"
      ],
      "metadata": {
        "id": "VLH_FIQwuZUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cop.plot_crime_occurrences_by_hour()"
      ],
      "metadata": {
        "id": "w-3tsyBCueOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_subplots_of_categories_by_year(year, df, top=12):\n",
        "    \"\"\"Density map subplots to show the top crimes occurred based on the year\"\"\"\n",
        "    \n",
        "    # San Francisco coordinates\n",
        "    clat = 37.773972\n",
        "    clon = -122.431297\n",
        "    \n",
        "    # select top 20 based on the frequency\n",
        "    sf_ = df[df['year'] == year]\n",
        "    category_vc = sf_['category'].value_counts().to_frame()\n",
        "    categories = category_vc.index.to_list()[:top]\n",
        "    \n",
        "    # subplots grid\n",
        "    nrows = 4; ncols = 3\n",
        "    fig = make_subplots(\n",
        "        rows=nrows, cols=ncols, subplot_titles=categories,\n",
        "        specs=[[{\"type\" : \"mapbox\"} for i in range(ncols)] for j in range(nrows)]\n",
        "    )\n",
        "\n",
        "    r = 1; c = 1\n",
        "    for name in categories:\n",
        "        group = sf_[sf_['category'] == name]\n",
        "        if (c > ncols):\n",
        "            r += 1\n",
        "            if (r > nrows): break\n",
        "            c = 1\n",
        "        f = go.Densitymapbox(lat=group['latitude'], lon=group['longitude'], radius=1)\n",
        "        fig.add_trace(trace=f, row=r, col=c)\n",
        "        c += 1\n",
        "    \n",
        "    fig.update_layout(\n",
        "        # autosize=True,\n",
        "        title=year,\n",
        "        height=1000, hovermode='closest', showlegend=False,\n",
        "        margin=dict(l=0, r=0, t=60, b=0)\n",
        "    )\n",
        "\n",
        "    fig.update_mapboxes(\n",
        "        center=dict(lat=clat, lon=clon),\n",
        "        bearing=0, pitch=0, zoom=10,\n",
        "        style='carto-positron'\n",
        "    )\n",
        "    \n",
        "    fig.show()\n",
        "    return None"
      ],
      "metadata": {
        "id": "1Kz-zMcSueSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make_subplots_of_categories_by_year(year=2003, df=train_sf_df)"
      ],
      "metadata": {
        "id": "vEGHlUYRueV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_categories_numericals(df):\n",
        "    \"\"\"Identifying the numerical and categorical columns separately\"\"\"\n",
        "    cols = list(df.columns)\n",
        "    num_cols = list(df._get_numeric_data().columns)\n",
        "    cate_cols = list(set(cols) - set(num_cols))\n",
        "    return cate_cols, num_cols"
      ],
      "metadata": {
        "id": "0dqNFs6kueZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ignore_columns = ['category', 'time', 'address', 'date']\n",
        "\n",
        "def extract_feature_dummies(df, column):\n",
        "    \"\"\"One-Hot-Encoding using Pandas\"\"\"\n",
        "    col_df = df[column]\n",
        "    return pd.get_dummies(data=col_df)\n",
        "\n",
        "def encode_multiple_columns(df, ignore_columns=ignore_columns):\n",
        "    \"\"\"Encoding the multiple columns and vertical stacking them\"\"\"\n",
        "    cate_cols, num_cols = split_categories_numericals(df=df)\n",
        "    \n",
        "    multi_feature_dummies = [df[num_cols]]\n",
        "    for i in cate_cols:\n",
        "        if i not in ignore_columns:\n",
        "            d = extract_feature_dummies(df=df, column=i)\n",
        "            multi_feature_dummies.append(d)\n",
        "\n",
        "    encoded_data = pd.concat(multi_feature_dummies, axis=1)\n",
        "    \n",
        "    return encoded_data"
      ],
      "metadata": {
        "id": "ftCDtlVouZcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_data = encode_multiple_columns(df=train_sf_df)"
      ],
      "metadata": {
        "id": "JNrtH09Lu052"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sf_pstations_tourists = {\n",
        "    \"sfpd\"                : [37.7725, -122.3894],\n",
        "    \"ingleside\"           : [37.7247, -122.4463],\n",
        "    \"central\"             : [37.7986, -122.4101],\n",
        "    \"northern\"            : [37.7802, -122.4324],\n",
        "    \"mission\"             : [37.7628, -122.4220],\n",
        "    \"tenderloin\"          : [37.7838, -122.4129],\n",
        "    \"taraval\"             : [37.7437, -122.4815],\n",
        "    \"sfpd park\"           : [37.7678, -122.4552],\n",
        "    \"bayview\"             : [37.7298, -122.3977],\n",
        "    \"kma438 sfpd\"         : [37.7725, -122.3894],\n",
        "    \"richmond\"            : [37.7801, -122.4644],\n",
        "    \"police commission\"   : [37.7725, -122.3894],\n",
        "    \"juvenile\"            : [37.7632, -122.4220],\n",
        "    \"southern\"            : [37.6556, -122.4366],\n",
        "    \"sfpd pistol range\"   : [37.7200, -122.4996],\n",
        "    \"sfpd public affairs\" : [37.7754, -122.4039],\n",
        "    \"broadmoor\"           : [37.6927, -122.4748],\n",
        "    #################\n",
        "    \"napa wine country\"      : [38.2975, -122.2869],\n",
        "    \"sonoma wine country\"    : [38.2919, -122.4580],\n",
        "    \"muir woods\"             : [37.8970, -122.5811],\n",
        "    \"golden gate\"            : [37.8199, -122.4783],\n",
        "    \"yosemite national park\" : [37.865101, -119.538330],\n",
        "}"
      ],
      "metadata": {
        "id": "i2vvIRiNu0_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_distance(ij):\n",
        "    \"\"\"Get distance from two coordinates\"\"\"\n",
        "    i = ij[0]\n",
        "    j = ij[1]\n",
        "    distance = haversine_distance(origin=i, destination=j)\n",
        "    return distance\n",
        "\n",
        "def extract_spatial_distance_feature(df, lat_column, lon_column, pname, pcoords):\n",
        "    \"\"\"Compute the distance between pcoords and all the feature values\"\"\"\n",
        "    lat_vals = df[lat_column].to_list()\n",
        "    lon_vals = df[lon_column].to_list()\n",
        "    \n",
        "    df_coords = list(zip(lat_vals, lon_vals))\n",
        "    pcoords_df_coords_combines = zip([pcoords] * len(df), df_coords)\n",
        "    \n",
        "    f = pd.DataFrame()\n",
        "    distances = list(map(get_distance, pcoords_df_coords_combines))\n",
        "    f[pname] = distances\n",
        "    \n",
        "    return f"
      ],
      "metadata": {
        "id": "1en9OeSou5-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_spatial_distance_multi_features(df, lat_column, lon_column, stations=sf_pstations_tourists):\n",
        "    \"\"\"Compute the spatial distance for multiple features and vertical stacking them\"\"\"\n",
        "    sfeatures = []\n",
        "    \n",
        "    for pname, pcoords in stations.items():\n",
        "        print(pname, pcoords)\n",
        "        sf = extract_spatial_distance_feature(df, lat_column, lon_column, pname, pcoords)\n",
        "        sfeatures.append(sf)\n",
        "    \n",
        "    spatial_distances = pd.concat(sfeatures, axis=1)\n",
        "    return spatial_distances"
      ],
      "metadata": {
        "id": "D9VhHidVu6Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sd_features = extract_spatial_distance_multi_features(df=train_sf_df, lat_column='latitude', lon_column='longitude')"
      ],
      "metadata": {
        "id": "FUQKWzD8vCzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lat_lon_sum(ll):\n",
        "    \"\"\"Return the sum of lat and lon\"\"\"\n",
        "    lat = ll[0]\n",
        "    lon = ll[1]\n",
        "    return lat + lon\n",
        "\n",
        "def lat_lon_diff(ll):\n",
        "    \"\"\"Return the diff of lat and lon\"\"\"\n",
        "    lat = ll[0]\n",
        "    lon = ll[1]\n",
        "    return lon - lat\n",
        "\n",
        "def lat_lon_sum_square(ll):\n",
        "    \"\"\"Return the square of sum of lat and lon\"\"\"\n",
        "    lat = ll[0]\n",
        "    lon = ll[1]\n",
        "    return (lat + lon) ** 2\n",
        "\n",
        "def lat_lon_diff_square(ll):\n",
        "    \"\"\"Return the square of diff of lat and lon\"\"\"\n",
        "    lat = ll[0]\n",
        "    lon = ll[1]\n",
        "    return (lat - lon) ** 2\n",
        "\n",
        "def lat_lon_sum_sqrt(ll):\n",
        "    \"\"\"Return the sqrt of sum of lat and lon\"\"\"\n",
        "    lat = ll[0]\n",
        "    lon = ll[1]\n",
        "    return (lat**2 + lon**2) ** (1 / 2)\n",
        "\n",
        "def lat_lon_diff_sqrt(ll):\n",
        "    \"\"\"Return the sqrt of diff of lat and lon\"\"\"\n",
        "    lat = ll[0]\n",
        "    lon = ll[1]\n",
        "    return (lon**2 - lat**2) ** (1 / 2)"
      ],
      "metadata": {
        "id": "PPnIfAF5vFdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sll_features = features_by_lat_lon(df=train_sf_df, lat_column='latitude', lon_column='longitude')"
      ],
      "metadata": {
        "id": "FQ_803_4vFgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bow_vectorizer(df, column, target='category', write_vect=True, kbest=20):\n",
        "    \"\"\"We should only fit on training data to avoid data leakage\"\"\"\n",
        "\n",
        "    model_name = 'vect_bow_{}.pkl'.format(column)\n",
        "    print(model_name)\n",
        "    df_col_val = df[column]\n",
        "\n",
        "    if not os.path.isfile(path=project_path + 'models/' + model_name):\n",
        "        vect = CountVectorizer()\n",
        "        vect.fit(raw_documents=df_col_val)\n",
        "        pickle.dump(vect, open(project_path + 'models/' + model_name, \"wb\"))\n",
        "        df_col_features = vect.transform(raw_documents=df_col_val)\n",
        "    else:\n",
        "        print(\"Model already exists in the directory.\")\n",
        "        vect = pickle.load(open(project_path + 'models/' + model_name, \"rb\"))\n",
        "        df_col_features = vect.transform(raw_documents=df_col_val)\n",
        "\n",
        "    if kbest:\n",
        "        fs = SelectKBest(k=kbest)\n",
        "        fs.fit(df_col_features, df[target])\n",
        "        df_col_features = fs.transform(df_col_features)\n",
        "    \n",
        "    return pd.DataFrame(df_col_features.toarray())"
      ],
      "metadata": {
        "id": "E8ywdxlqvFkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_address_bow = create_bow_vectorizer(df=train_sf_df, column='address')"
      ],
      "metadata": {
        "id": "aAVYhFx0vKUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tfidf_vectorizer(df, column, target='category', write_vect=True, kbest=20):\n",
        "    \"\"\"We should only fit on training data to avoid data leakage\"\"\"\n",
        "\n",
        "    model_name = 'vect_tfidf_{}.pkl'.format(column)\n",
        "    print(model_name)\n",
        "    df_col_val = df[column]\n",
        "\n",
        "    if not os.path.isfile(path=project_path + 'models/' + model_name):\n",
        "        vect = TfidfVectorizer()\n",
        "        vect.fit(raw_documents=df_col_val)\n",
        "        pickle.dump(vect, open(project_path + 'models/' + model_name, \"wb\"))\n",
        "        df_col_features = vect.transform(raw_documents=df_col_val)\n",
        "    else:\n",
        "        print(\"Model already exists in the directory.\")\n",
        "        vect = pickle.load(open(project_path + 'models/' + model_name, \"rb\"))\n",
        "        df_col_features = vect.transform(raw_documents=df_col_val)\n",
        "\n",
        "    if kbest:\n",
        "        fs = SelectKBest(k=kbest)\n",
        "        fs.fit(df_col_features, df[target])\n",
        "        df_col_features = fs.transform(df_col_features)\n",
        "    \n",
        "    return pd.DataFrame(df_col_features.toarray())"
      ],
      "metadata": {
        "id": "2JJ29ho8vKZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_address_tfidf = create_tfidf_vectorizer(df=train_sf_df, column='address')"
      ],
      "metadata": {
        "id": "Rp-axgmzvSCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sf_df_featurized = pd.concat([encoded_data, sd_features, sll_features, train_address_bow, train_address_tfidf], axis=1)\n",
        "train_sf_df_featurized['category'] = train_sf_df['category']"
      ],
      "metadata": {
        "id": "okRUYHhivSK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sf_df_featurized.shape"
      ],
      "metadata": {
        "id": "D80YJmsdvWkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_by_stratification(df, target):\n",
        "    \"\"\"Apply stratification and split the data\"\"\"\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, stratify=y, random_state=42)\n",
        "    return X_train, y_train"
      ],
      "metadata": {
        "id": "TYzwbZkAvWob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_tsne(X, y, k=20, perplexity=50):\n",
        "    \"\"\"TSNE plot after reducing the dimensionality to k best features\"\"\"\n",
        "    \n",
        "    fs = SelectKBest(k=k)\n",
        "    fs.fit(X, y)\n",
        "    X = fs.transform(X)\n",
        "    print(X.shape)\n",
        "    \n",
        "    X = StandardScaler().fit_transform(X)\n",
        "    tsne = TSNE(n_components=2, random_state=0, perplexity=perplexity)\n",
        "    projections = tsne.fit_transform(X, )\n",
        "    \n",
        "    fig = px.scatter(projections, x=0, y=1, color=y)\n",
        "    fig.update_layout(\n",
        "        autosize=True,\n",
        "        height=600,\n",
        "        hovermode='closest',\n",
        "        showlegend=True,\n",
        "        margin=dict(l=10, r=10, t=30, b=0)\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "rogxk-8kvWtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = divide_by_stratification(df=train_sf_df_featurized, target='category')"
      ],
      "metadata": {
        "id": "sPDYbFMRvdW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tsne(X=X_train, y=y_train)"
      ],
      "metadata": {
        "id": "q2oY-yqGvdac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segregate_only_top(df, column, n=12, randomize=True):\n",
        "    \"\"\"Considering only top crimes and randomizing the data\"\"\"\n",
        "    top_n = df[column].value_counts().index.to_list()[:n]\n",
        "    \n",
        "    df_vals = []\n",
        "    for i in top_n:\n",
        "        df_vals.append(df[df[column] == i])\n",
        "    \n",
        "    df = pd.concat(df_vals, axis=0)\n",
        "    if randomize:\n",
        "        df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Y-Po9UHxvdjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = segregate_only_top(df=train_sf_df_featurized, column='category')\n",
        "X_train, y_train = divide_by_stratification(df=data, target='category')"
      ],
      "metadata": {
        "id": "HTaZ1Shrvi4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tsne(X=X_train, y=y_train)"
      ],
      "metadata": {
        "id": "LtI-ybLMvi8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = segregate_only_top(df=train_sf_df_featurized, column='category', n=5)\n",
        "X_train, y_train = divide_by_stratification(df=data, target='category')"
      ],
      "metadata": {
        "id": "gvGfLBYQvjAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tsne(X=X_train, y=y_train)"
      ],
      "metadata": {
        "id": "HUCq_zw1vrHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}